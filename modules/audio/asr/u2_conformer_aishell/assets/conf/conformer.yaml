data:
  train_manifest: data/manifest.train
  dev_manifest: data/manifest.dev
  test_manifest: data/manifest.test
  min_input_len: 0.5
  max_input_len: 20.0 # second
  min_output_len: 0.0
  max_output_len: 400.0
  min_output_input_ratio: 0.05
  max_output_input_ratio: 10.0

collator:
  vocab_filepath: data/vocab.txt
  unit_type: 'char'
  spm_model_prefix: ''
  augmentation_config: conf/augmentation.json
  batch_size: 64
  raw_wav: True  # use raw_wav or kaldi feature
  spectrum_type: fbank #linear, mfcc, fbank
  feat_dim: 80
  delta_delta: False
  dither: 1.0
  target_sample_rate: 16000
  max_freq: None
  n_fft: None
  stride_ms: 10.0
  window_ms: 25.0
  use_dB_normalization: False
  target_dB: -20
  random_seed: 0
  keep_transcription_text: False
  sortagrad: True
  shuffle_method: batch_shuffle
  num_workers: 2

decoding:
  alpha: 2.5
  batch_size: 128
  beam_size: 10
  beta: 0.3
  ctc_weight: 0.0
  cutoff_prob: 1.0
  cutoff_top_n: 0
  decoding_chunk_size: -1
  decoding_method: attention
  error_rate_type: cer
  lang_model_path: data/lm/common_crawl_00.prune01111.trie.klm
  num_decoding_left_chunks: -1
  num_proc_bsearch: 8
  simulate_streaming: False
model:
  cmvn_file: data/mean_std.json
  cmvn_file_type: json
  decoder: transformer
  decoder_conf:
    attention_heads: 4
    dropout_rate: 0.1
    linear_units: 2048
    num_blocks: 6
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0
  encoder: conformer
  encoder_conf:
    activation_type: swish
    attention_dropout_rate: 0.0
    attention_heads: 4
    cnn_module_kernel: 15
    dropout_rate: 0.1
    input_layer: conv2d
    linear_units: 2048
    normalize_before: True
    num_blocks: 12
    output_size: 256
    pos_enc_layer_type: rel_pos
    positional_dropout_rate: 0.1
    selfattention_layer_type: rel_selfattn
    use_cnn_module: True
  input_dim: 0
  model_conf:
    ctc_weight: 0.3
    ctc_dropoutrate: 0.0
    ctc_grad_norm_type: instance
    length_normalized_loss: False
    lsm_weight: 0.1
  output_dim: 0
training:
  accum_grad: 2
  global_grad_clip: 5.0
  log_interval: 100
  n_epoch: 300
  optim: adam
  optim_conf:
    lr: 0.002
    weight_decay: 1e-06
  scheduler: warmuplr
  scheduler_conf:
    lr_decay: 1.0
    warmup_steps: 25000
  checkpoint:
    kbest_n: 50
    latest_n: 5
